{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ExLinReg",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0Rl0vCvhXkA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "2efbb0f1-edef-4077-81c7-564fae5179e7"
      },
      "source": [
        "#linear regression model\n",
        "\n",
        "import torch.nn as nn \n",
        "import torch\n",
        "\n",
        "inputs = torch.randn(10, 4)\n",
        "#print(inputs)\n",
        "weights = torch.randn(4, 1, requires_grad = True)\n",
        "#print(weights)\n",
        "truths = torch.randn(10, 1)\n",
        "#print(truths)\n",
        "predictions = torch.matmul(inputs, weights)\n",
        "#print(predictions)\n",
        "\n",
        "myloss = torch.sum((truths-predictions)**2)/10\n",
        "#print(myloss)\n",
        "myloss = 0\n",
        "for i in range(10):\n",
        "  myloss += (predictions[i][0] - truths[i][0])**2\n",
        "myloss /= 10\n",
        "#print(myloss)\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "learning_rate = 0.01\n",
        "\n",
        "\n",
        "#vanilla version optimization\n",
        "for i in range(5):\n",
        "  predictions = torch.matmul(inputs, weights)\n",
        "  output = loss(predictions, truths)\n",
        "  output.backward()\n",
        "  with torch.no_grad():\n",
        "    weights -= learning_rate*weights.grad\n",
        "  print(output)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(5.6177, grad_fn=<MseLossBackward>)\n",
            "tensor(5.3884, grad_fn=<MseLossBackward>)\n",
            "tensor(4.9565, grad_fn=<MseLossBackward>)\n",
            "tensor(4.3715, grad_fn=<MseLossBackward>)\n",
            "tensor(3.6995, grad_fn=<MseLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcaJ_aCvnZCE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "outputId": "14ebd844-f2ae-4d3a-df23-d80ce4624f7e"
      },
      "source": [
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(4,5),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(5,1)\n",
        "    #linear means fully connected, sequential means regression, relu is our hidden layer\n",
        ")\n",
        "\n",
        "#optimizers are functions that improve your weights\n",
        "optimizer = torch.optim.Adagrad(model.parameters(), lr = learning_rate)\n",
        "\n",
        "for i in range(5):\n",
        "  predictions = model(inputs)\n",
        "  output = loss(predictions, truths)\n",
        "  model.zero_grad()\n",
        "  output.backward()\n",
        "  optimizer.step()\n",
        "  print(output)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.6573, grad_fn=<MseLossBackward>)\n",
            "tensor(0.6407, grad_fn=<MseLossBackward>)\n",
            "tensor(0.6296, grad_fn=<MseLossBackward>)\n",
            "tensor(0.6207, grad_fn=<MseLossBackward>)\n",
            "tensor(0.6131, grad_fn=<MseLossBackward>)\n",
            "tensor([[0.4273],\n",
            "        [0.4376],\n",
            "        [0.5272],\n",
            "        [0.2585],\n",
            "        [0.5163],\n",
            "        [0.4661],\n",
            "        [0.4819],\n",
            "        [0.4153],\n",
            "        [0.4684],\n",
            "        [0.4504]], grad_fn=<SigmoidBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}